{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Language_modeling.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ylS_WDHSFPEh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 921
        },
        "outputId": "92a9f5ef-ebc1-4357-9e24-15a088eaff99"
      },
      "source": [
        "!pip install tensorflow==1.15.0"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow==1.15.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3f/98/5a99af92fb911d7a88a0005ad55005f35b4c1ba8d75fba02df726cd936e6/tensorflow-1.15.0-cp36-cp36m-manylinux2010_x86_64.whl (412.3MB)\n",
            "\u001b[K     |████████████████████████████████| 412.3MB 39kB/s \n",
            "\u001b[?25hRequirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.0) (1.1.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.0) (1.12.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.0) (3.2.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.0) (1.18.2)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.0) (0.2.0)\n",
            "Collecting tensorboard<1.16.0,>=1.15.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1e/e9/d3d747a97f7188f48aa5eda486907f3b345cd409f0a0850468ba867db246/tensorboard-1.15.0-py3-none-any.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 46.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.0) (1.27.2)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.0) (0.34.2)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.0) (0.9.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.0) (0.8.1)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.0) (3.10.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.0) (1.1.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.0) (1.0.8)\n",
            "Collecting gast==0.2.2\n",
            "  Downloading https://files.pythonhosted.org/packages/4e/35/11749bf99b2d4e3cceb4d55ca22590b0d7c2c62b9de38ac4a4a7f4687421/gast-0.2.2.tar.gz\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.0) (1.12.0)\n",
            "Collecting tensorflow-estimator==1.15.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/de/62/2ee9cd74c9fa2fa450877847ba560b260f5d0fb70ee0595203082dafcc9d/tensorflow_estimator-1.15.1-py2.py3-none-any.whl (503kB)\n",
            "\u001b[K     |████████████████████████████████| 512kB 43.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.0) (3.2.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.0) (46.0.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.0) (1.0.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow==1.15.0) (2.10.0)\n",
            "Building wheels for collected packages: gast\n",
            "  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gast: filename=gast-0.2.2-cp36-none-any.whl size=7540 sha256=6d2918158f165d62cd89aa0dc254ae3e32fa83aa8be3092357dc82b11438f80c\n",
            "  Stored in directory: /root/.cache/pip/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd\n",
            "Successfully built gast\n",
            "Installing collected packages: tensorboard, gast, tensorflow-estimator, tensorflow\n",
            "  Found existing installation: tensorboard 2.1.1\n",
            "    Uninstalling tensorboard-2.1.1:\n",
            "      Successfully uninstalled tensorboard-2.1.1\n",
            "  Found existing installation: gast 0.3.3\n",
            "    Uninstalling gast-0.3.3:\n",
            "      Successfully uninstalled gast-0.3.3\n",
            "  Found existing installation: tensorflow-estimator 2.2.0rc0\n",
            "    Uninstalling tensorflow-estimator-2.2.0rc0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.2.0rc0\n",
            "  Found existing installation: tensorflow 2.2.0rc1\n",
            "    Uninstalling tensorflow-2.2.0rc1:\n",
            "      Successfully uninstalled tensorflow-2.2.0rc1\n",
            "Successfully installed gast-0.2.2 tensorboard-1.15.0 tensorflow-1.15.0 tensorflow-estimator-1.15.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "gast",
                  "tensorboard",
                  "tensorflow"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SlEntaukHinS",
        "colab_type": "code",
        "outputId": "87468959-3d1a-4995-fa52-2a9240febca2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "import numpy as np\n",
        "import string"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mM8WaYWQJP3o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# File Reading\n",
        "with open('/content/alice_in_wonderland.txt','r') as content_file:\n",
        "  content = content_file.read()\n",
        "content2 = \" \".join(\"\".join([\" \"if ch in string.punctuation else ch for ch in content]).split())\n",
        "tokens = nltk.word_tokenize(content2)\n",
        "tokens = [word.lower() for word in tokens if len(word)>=2]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O4BCmrFQKr1K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# N value = 3 for N grams, in which N-1 are used to predict last Nth word\n",
        "N = 3\n",
        "quads = list(nltk.ngrams(tokens,N))\n",
        "newl_app = []\n",
        "for ln in quads:\n",
        "  newl = \" \".join(ln)\n",
        "  newl_app.append(newl)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2qNxq0O-LZTG",
        "colab_type": "code",
        "outputId": "d232eaea-9e33-47f5-ec59-4adff07a444d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# Vectorizing the words\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "vectorizer = CountVectorizer()\n",
        "x_trigm = []\n",
        "y_trigm = []\n",
        "for l in newl_app:\n",
        "  x_str = \" \".join(l.split()[0:N-1])\n",
        "  y_str = l.split()[N-1]\n",
        "  x_trigm.append(x_str)\n",
        "  y_trigm.append(y_str)\n",
        "x_trigm_check = vectorizer.fit_transform(x_trigm).todense()\n",
        "y_trigm_check = vectorizer.fit_transform(y_trigm).todense()\n",
        "# Dictionaries from word to integer and integer to word \n",
        "dictnry = vectorizer.vocabulary_\n",
        "rev_dictnry = {v:k for k,v in dictnry.items()}\n",
        "X = np.array(x_trigm_check)\n",
        "Y = np.array(y_trigm_check)\n",
        "Xtrain, Xtest, Ytrain, Ytest,xtrain_tg,xtest_tg = train_test_split(X, Y,x_trigm, test_size=0.3,random_state=42)\n",
        "print(\"X Train shape\",Xtrain.shape, \"Y Train shape\" , Ytrain.shape)\n",
        "print(\"X Test shape\",Xtest.shape, \"Y Test shape\" , Ytest.shape)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X Train shape (17944, 2562) Y Train shape (17944, 2562)\n",
            "X Test shape (7691, 2562) Y Test shape (7691, 2562)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2xLSf--BNQi-",
        "colab_type": "code",
        "outputId": "5fe4fd94-8584-4471-cbf6-16bee2efb55b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        }
      },
      "source": [
        "# Model \n",
        "from keras.layers import Input,Dense,Dropout\n",
        "from keras.models import Model\n",
        "np.random.seed(42)\n",
        "BATCH_SIZE = 128\n",
        "NUM_EPOCHS = 100\n",
        "input_layer = Input(shape = (Xtrain.shape[1],),name=\"input\")\n",
        "first_layer = Dense(1000,activation='relu',name = \"first\")(input_layer)\n",
        "first_dropout = Dropout(0.5,name=\"firstdout\")(first_layer)\n",
        "second_layer = Dense(800,activation='relu',name=\"second\") (first_dropout)\n",
        "third_layer = Dense(1000,activation='relu',name=\"third\") (second_layer)\n",
        "third_dropout = Dropout(0.5,name=\"thirdout\")(third_layer)\n",
        "fourth_layer = Dense(Ytrain.shape[1],activation='softmax',name = \"fourth\")(third_dropout)\n",
        "model = Model(input_layer,fourth_layer)\n",
        "model.compile(optimizer = \"adam\",loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "print (model.summary())"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input (InputLayer)           (None, 2562)              0         \n",
            "_________________________________________________________________\n",
            "first (Dense)                (None, 1000)              2563000   \n",
            "_________________________________________________________________\n",
            "firstdout (Dropout)          (None, 1000)              0         \n",
            "_________________________________________________________________\n",
            "second (Dense)               (None, 800)               800800    \n",
            "_________________________________________________________________\n",
            "third (Dense)                (None, 1000)              801000    \n",
            "_________________________________________________________________\n",
            "thirdout (Dropout)           (None, 1000)              0         \n",
            "_________________________________________________________________\n",
            "fourth (Dense)               (None, 2562)              2564562   \n",
            "=================================================================\n",
            "Total params: 6,729,362\n",
            "Trainable params: 6,729,362\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J8hlyWDzNqdE",
        "colab_type": "code",
        "outputId": "a7bf0098-e99b-410b-d154-85114b663fb0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "history = model.fit(Xtrain, Ytrain, batch_size=BATCH_SIZE,epochs=NUM_EPOCHS, verbose=1,validation_split = 0.2)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 14355 samples, validate on 3589 samples\n",
            "Epoch 1/100\n",
            "14355/14355 [==============================] - 2s 159us/step - loss: 6.3265 - acc: 0.0567 - val_loss: 6.0697 - val_acc: 0.0607\n",
            "Epoch 2/100\n",
            "14355/14355 [==============================] - 2s 125us/step - loss: 5.9152 - acc: 0.0688 - val_loss: 6.0358 - val_acc: 0.0627\n",
            "Epoch 3/100\n",
            "14355/14355 [==============================] - 2s 125us/step - loss: 5.7058 - acc: 0.0791 - val_loss: 5.9856 - val_acc: 0.0853\n",
            "Epoch 4/100\n",
            "14355/14355 [==============================] - 2s 124us/step - loss: 5.4673 - acc: 0.1041 - val_loss: 5.9346 - val_acc: 0.0950\n",
            "Epoch 5/100\n",
            "14355/14355 [==============================] - 2s 126us/step - loss: 5.2131 - acc: 0.1311 - val_loss: 5.9243 - val_acc: 0.0995\n",
            "Epoch 6/100\n",
            "14355/14355 [==============================] - 2s 126us/step - loss: 4.9490 - acc: 0.1524 - val_loss: 5.9567 - val_acc: 0.1064\n",
            "Epoch 7/100\n",
            "14355/14355 [==============================] - 2s 126us/step - loss: 4.7266 - acc: 0.1767 - val_loss: 5.9612 - val_acc: 0.1078\n",
            "Epoch 8/100\n",
            "14355/14355 [==============================] - 2s 126us/step - loss: 4.4940 - acc: 0.1958 - val_loss: 6.0811 - val_acc: 0.1059\n",
            "Epoch 9/100\n",
            "14355/14355 [==============================] - 2s 127us/step - loss: 4.2532 - acc: 0.2231 - val_loss: 6.1626 - val_acc: 0.1062\n",
            "Epoch 10/100\n",
            "14355/14355 [==============================] - 2s 128us/step - loss: 4.0741 - acc: 0.2449 - val_loss: 6.2406 - val_acc: 0.1087\n",
            "Epoch 11/100\n",
            "14355/14355 [==============================] - 2s 129us/step - loss: 3.8693 - acc: 0.2657 - val_loss: 6.3161 - val_acc: 0.1081\n",
            "Epoch 12/100\n",
            "14355/14355 [==============================] - 2s 127us/step - loss: 3.6890 - acc: 0.2807 - val_loss: 6.3975 - val_acc: 0.1076\n",
            "Epoch 13/100\n",
            "14355/14355 [==============================] - 2s 127us/step - loss: 3.5125 - acc: 0.3074 - val_loss: 6.4809 - val_acc: 0.1148\n",
            "Epoch 14/100\n",
            "14355/14355 [==============================] - 2s 125us/step - loss: 3.3612 - acc: 0.3214 - val_loss: 6.5514 - val_acc: 0.1123\n",
            "Epoch 15/100\n",
            "14355/14355 [==============================] - 2s 126us/step - loss: 3.2200 - acc: 0.3403 - val_loss: 6.6823 - val_acc: 0.1101\n",
            "Epoch 16/100\n",
            "14355/14355 [==============================] - 2s 126us/step - loss: 3.0604 - acc: 0.3580 - val_loss: 6.7666 - val_acc: 0.1009\n",
            "Epoch 17/100\n",
            "14355/14355 [==============================] - 2s 128us/step - loss: 2.9321 - acc: 0.3709 - val_loss: 6.8209 - val_acc: 0.1106\n",
            "Epoch 18/100\n",
            "14355/14355 [==============================] - 2s 124us/step - loss: 2.8032 - acc: 0.3859 - val_loss: 6.9411 - val_acc: 0.1095\n",
            "Epoch 19/100\n",
            "14355/14355 [==============================] - 2s 124us/step - loss: 2.6921 - acc: 0.4036 - val_loss: 7.0377 - val_acc: 0.1067\n",
            "Epoch 20/100\n",
            "14355/14355 [==============================] - 2s 124us/step - loss: 2.5659 - acc: 0.4199 - val_loss: 7.1491 - val_acc: 0.1103\n",
            "Epoch 21/100\n",
            "14355/14355 [==============================] - 2s 125us/step - loss: 2.4950 - acc: 0.4265 - val_loss: 7.2245 - val_acc: 0.1092\n",
            "Epoch 22/100\n",
            "14355/14355 [==============================] - 2s 125us/step - loss: 2.3857 - acc: 0.4458 - val_loss: 7.2356 - val_acc: 0.1123\n",
            "Epoch 23/100\n",
            "14355/14355 [==============================] - 2s 126us/step - loss: 2.2877 - acc: 0.4587 - val_loss: 7.3726 - val_acc: 0.1106\n",
            "Epoch 24/100\n",
            "14355/14355 [==============================] - 2s 124us/step - loss: 2.2102 - acc: 0.4690 - val_loss: 7.4200 - val_acc: 0.1073\n",
            "Epoch 25/100\n",
            "14355/14355 [==============================] - 2s 123us/step - loss: 2.1324 - acc: 0.4798 - val_loss: 7.5384 - val_acc: 0.1120\n",
            "Epoch 26/100\n",
            "14355/14355 [==============================] - 2s 125us/step - loss: 2.0584 - acc: 0.4967 - val_loss: 7.5403 - val_acc: 0.1115\n",
            "Epoch 27/100\n",
            "14355/14355 [==============================] - 2s 123us/step - loss: 2.0074 - acc: 0.5063 - val_loss: 7.5847 - val_acc: 0.1112\n",
            "Epoch 28/100\n",
            "14355/14355 [==============================] - 2s 125us/step - loss: 1.9471 - acc: 0.5147 - val_loss: 7.6587 - val_acc: 0.1109\n",
            "Epoch 29/100\n",
            "14355/14355 [==============================] - 2s 126us/step - loss: 1.8849 - acc: 0.5260 - val_loss: 7.6862 - val_acc: 0.1073\n",
            "Epoch 30/100\n",
            "14355/14355 [==============================] - 2s 125us/step - loss: 1.8338 - acc: 0.5361 - val_loss: 7.8994 - val_acc: 0.1081\n",
            "Epoch 31/100\n",
            "14355/14355 [==============================] - 2s 124us/step - loss: 1.7978 - acc: 0.5397 - val_loss: 7.8364 - val_acc: 0.1089\n",
            "Epoch 32/100\n",
            "14355/14355 [==============================] - 2s 125us/step - loss: 1.7576 - acc: 0.5435 - val_loss: 7.8629 - val_acc: 0.1078\n",
            "Epoch 33/100\n",
            "14355/14355 [==============================] - 2s 124us/step - loss: 1.7104 - acc: 0.5574 - val_loss: 7.9748 - val_acc: 0.1151\n",
            "Epoch 34/100\n",
            "14355/14355 [==============================] - 2s 125us/step - loss: 1.6834 - acc: 0.5569 - val_loss: 7.9562 - val_acc: 0.1039\n",
            "Epoch 35/100\n",
            "14355/14355 [==============================] - 2s 126us/step - loss: 1.6644 - acc: 0.5673 - val_loss: 8.0085 - val_acc: 0.1101\n",
            "Epoch 36/100\n",
            "14355/14355 [==============================] - 2s 125us/step - loss: 1.6250 - acc: 0.5698 - val_loss: 7.9738 - val_acc: 0.1053\n",
            "Epoch 37/100\n",
            "14355/14355 [==============================] - 2s 127us/step - loss: 1.5977 - acc: 0.5750 - val_loss: 8.0179 - val_acc: 0.1053\n",
            "Epoch 38/100\n",
            "14355/14355 [==============================] - 2s 125us/step - loss: 1.5666 - acc: 0.5765 - val_loss: 8.0352 - val_acc: 0.1073\n",
            "Epoch 39/100\n",
            "14355/14355 [==============================] - 2s 127us/step - loss: 1.5558 - acc: 0.5814 - val_loss: 8.1340 - val_acc: 0.1087\n",
            "Epoch 40/100\n",
            "14355/14355 [==============================] - 2s 125us/step - loss: 1.5203 - acc: 0.5884 - val_loss: 8.1744 - val_acc: 0.1076\n",
            "Epoch 41/100\n",
            "14355/14355 [==============================] - 2s 125us/step - loss: 1.5038 - acc: 0.5889 - val_loss: 8.1674 - val_acc: 0.1064\n",
            "Epoch 42/100\n",
            "14355/14355 [==============================] - 2s 124us/step - loss: 1.4834 - acc: 0.5930 - val_loss: 8.2094 - val_acc: 0.1050\n",
            "Epoch 43/100\n",
            "14355/14355 [==============================] - 2s 124us/step - loss: 1.4679 - acc: 0.5933 - val_loss: 8.2566 - val_acc: 0.1081\n",
            "Epoch 44/100\n",
            "14355/14355 [==============================] - 2s 125us/step - loss: 1.4552 - acc: 0.5934 - val_loss: 8.2505 - val_acc: 0.1106\n",
            "Epoch 45/100\n",
            "14355/14355 [==============================] - 2s 125us/step - loss: 1.4345 - acc: 0.5986 - val_loss: 8.3259 - val_acc: 0.1087\n",
            "Epoch 46/100\n",
            "14355/14355 [==============================] - 2s 125us/step - loss: 1.4338 - acc: 0.5964 - val_loss: 8.3515 - val_acc: 0.1126\n",
            "Epoch 47/100\n",
            "14355/14355 [==============================] - 2s 125us/step - loss: 1.4099 - acc: 0.6031 - val_loss: 8.3302 - val_acc: 0.1087\n",
            "Epoch 48/100\n",
            "14355/14355 [==============================] - 2s 124us/step - loss: 1.3802 - acc: 0.6102 - val_loss: 8.4070 - val_acc: 0.1073\n",
            "Epoch 49/100\n",
            "14355/14355 [==============================] - 2s 124us/step - loss: 1.3994 - acc: 0.6040 - val_loss: 8.3429 - val_acc: 0.1067\n",
            "Epoch 50/100\n",
            "14355/14355 [==============================] - 2s 125us/step - loss: 1.3549 - acc: 0.6135 - val_loss: 8.4812 - val_acc: 0.1109\n",
            "Epoch 51/100\n",
            "14355/14355 [==============================] - 2s 125us/step - loss: 1.3484 - acc: 0.6103 - val_loss: 8.3667 - val_acc: 0.1076\n",
            "Epoch 52/100\n",
            "14355/14355 [==============================] - 2s 124us/step - loss: 1.3441 - acc: 0.6099 - val_loss: 8.4227 - val_acc: 0.1081\n",
            "Epoch 53/100\n",
            "14355/14355 [==============================] - 2s 125us/step - loss: 1.3254 - acc: 0.6148 - val_loss: 8.4332 - val_acc: 0.1070\n",
            "Epoch 54/100\n",
            "14355/14355 [==============================] - 2s 124us/step - loss: 1.3165 - acc: 0.6151 - val_loss: 8.5117 - val_acc: 0.1098\n",
            "Epoch 55/100\n",
            "14355/14355 [==============================] - 2s 124us/step - loss: 1.3108 - acc: 0.6166 - val_loss: 8.4547 - val_acc: 0.1123\n",
            "Epoch 56/100\n",
            "14355/14355 [==============================] - 2s 124us/step - loss: 1.3038 - acc: 0.6180 - val_loss: 8.5811 - val_acc: 0.1062\n",
            "Epoch 57/100\n",
            "14355/14355 [==============================] - 2s 125us/step - loss: 1.3024 - acc: 0.6145 - val_loss: 8.5376 - val_acc: 0.1095\n",
            "Epoch 58/100\n",
            "14355/14355 [==============================] - 2s 126us/step - loss: 1.2937 - acc: 0.6143 - val_loss: 8.5290 - val_acc: 0.1095\n",
            "Epoch 59/100\n",
            "14355/14355 [==============================] - 2s 124us/step - loss: 1.2726 - acc: 0.6230 - val_loss: 8.5223 - val_acc: 0.1092\n",
            "Epoch 60/100\n",
            "14355/14355 [==============================] - 2s 127us/step - loss: 1.2730 - acc: 0.6222 - val_loss: 8.6036 - val_acc: 0.1073\n",
            "Epoch 61/100\n",
            "14355/14355 [==============================] - 2s 126us/step - loss: 1.2595 - acc: 0.6209 - val_loss: 8.6179 - val_acc: 0.1117\n",
            "Epoch 62/100\n",
            "14355/14355 [==============================] - 2s 126us/step - loss: 1.2656 - acc: 0.6189 - val_loss: 8.5685 - val_acc: 0.1120\n",
            "Epoch 63/100\n",
            "14355/14355 [==============================] - 2s 125us/step - loss: 1.2526 - acc: 0.6261 - val_loss: 8.5889 - val_acc: 0.1042\n",
            "Epoch 64/100\n",
            "14355/14355 [==============================] - 2s 125us/step - loss: 1.2396 - acc: 0.6262 - val_loss: 8.6446 - val_acc: 0.1106\n",
            "Epoch 65/100\n",
            "14355/14355 [==============================] - 2s 124us/step - loss: 1.2440 - acc: 0.6222 - val_loss: 8.6513 - val_acc: 0.1076\n",
            "Epoch 66/100\n",
            "14355/14355 [==============================] - 2s 124us/step - loss: 1.2426 - acc: 0.6226 - val_loss: 8.6634 - val_acc: 0.1098\n",
            "Epoch 67/100\n",
            "14355/14355 [==============================] - 2s 124us/step - loss: 1.2248 - acc: 0.6287 - val_loss: 8.6444 - val_acc: 0.1025\n",
            "Epoch 68/100\n",
            "14355/14355 [==============================] - 2s 126us/step - loss: 1.2183 - acc: 0.6290 - val_loss: 8.6850 - val_acc: 0.1056\n",
            "Epoch 69/100\n",
            "14355/14355 [==============================] - 2s 125us/step - loss: 1.2215 - acc: 0.6245 - val_loss: 8.6660 - val_acc: 0.1103\n",
            "Epoch 70/100\n",
            "14355/14355 [==============================] - 2s 126us/step - loss: 1.2136 - acc: 0.6256 - val_loss: 8.6802 - val_acc: 0.1070\n",
            "Epoch 71/100\n",
            "14355/14355 [==============================] - 2s 125us/step - loss: 1.1964 - acc: 0.6341 - val_loss: 8.7271 - val_acc: 0.1092\n",
            "Epoch 72/100\n",
            "14355/14355 [==============================] - 2s 125us/step - loss: 1.1858 - acc: 0.6320 - val_loss: 8.7247 - val_acc: 0.1106\n",
            "Epoch 73/100\n",
            "14355/14355 [==============================] - 2s 126us/step - loss: 1.1924 - acc: 0.6268 - val_loss: 8.6899 - val_acc: 0.1098\n",
            "Epoch 74/100\n",
            "14355/14355 [==============================] - 2s 127us/step - loss: 1.1898 - acc: 0.6308 - val_loss: 8.7281 - val_acc: 0.1101\n",
            "Epoch 75/100\n",
            "14355/14355 [==============================] - 2s 126us/step - loss: 1.1753 - acc: 0.6341 - val_loss: 8.7783 - val_acc: 0.1123\n",
            "Epoch 76/100\n",
            "14355/14355 [==============================] - 2s 127us/step - loss: 1.1733 - acc: 0.6349 - val_loss: 8.8511 - val_acc: 0.1070\n",
            "Epoch 77/100\n",
            "14355/14355 [==============================] - 2s 128us/step - loss: 1.1681 - acc: 0.6293 - val_loss: 8.7860 - val_acc: 0.1095\n",
            "Epoch 78/100\n",
            "14355/14355 [==============================] - 2s 126us/step - loss: 1.1660 - acc: 0.6363 - val_loss: 8.8479 - val_acc: 0.1106\n",
            "Epoch 79/100\n",
            "14355/14355 [==============================] - 2s 126us/step - loss: 1.1702 - acc: 0.6295 - val_loss: 8.8752 - val_acc: 0.1092\n",
            "Epoch 80/100\n",
            "14355/14355 [==============================] - 2s 124us/step - loss: 1.1480 - acc: 0.6366 - val_loss: 8.9350 - val_acc: 0.1123\n",
            "Epoch 81/100\n",
            "14355/14355 [==============================] - 2s 125us/step - loss: 1.1535 - acc: 0.6341 - val_loss: 8.9159 - val_acc: 0.1092\n",
            "Epoch 82/100\n",
            "14355/14355 [==============================] - 2s 125us/step - loss: 1.1419 - acc: 0.6362 - val_loss: 8.8739 - val_acc: 0.1126\n",
            "Epoch 83/100\n",
            "14355/14355 [==============================] - 2s 125us/step - loss: 1.1323 - acc: 0.6403 - val_loss: 8.8424 - val_acc: 0.1142\n",
            "Epoch 84/100\n",
            "14355/14355 [==============================] - 2s 125us/step - loss: 1.1469 - acc: 0.6350 - val_loss: 8.9713 - val_acc: 0.1165\n",
            "Epoch 85/100\n",
            "14355/14355 [==============================] - 2s 124us/step - loss: 1.1310 - acc: 0.6356 - val_loss: 8.8614 - val_acc: 0.1173\n",
            "Epoch 86/100\n",
            "14355/14355 [==============================] - 2s 125us/step - loss: 1.1317 - acc: 0.6366 - val_loss: 9.0174 - val_acc: 0.1137\n",
            "Epoch 87/100\n",
            "14355/14355 [==============================] - 2s 126us/step - loss: 1.1340 - acc: 0.6345 - val_loss: 8.9808 - val_acc: 0.1128\n",
            "Epoch 88/100\n",
            "14355/14355 [==============================] - 2s 127us/step - loss: 1.1278 - acc: 0.6366 - val_loss: 8.8561 - val_acc: 0.1128\n",
            "Epoch 89/100\n",
            "14355/14355 [==============================] - 2s 124us/step - loss: 1.1188 - acc: 0.6361 - val_loss: 8.9481 - val_acc: 0.1142\n",
            "Epoch 90/100\n",
            "14355/14355 [==============================] - 2s 125us/step - loss: 1.1239 - acc: 0.6378 - val_loss: 8.9871 - val_acc: 0.1120\n",
            "Epoch 91/100\n",
            "14355/14355 [==============================] - 2s 124us/step - loss: 1.1258 - acc: 0.6379 - val_loss: 8.9277 - val_acc: 0.1106\n",
            "Epoch 92/100\n",
            "14355/14355 [==============================] - 2s 126us/step - loss: 1.1161 - acc: 0.6375 - val_loss: 8.9271 - val_acc: 0.1067\n",
            "Epoch 93/100\n",
            "14355/14355 [==============================] - 2s 125us/step - loss: 1.1310 - acc: 0.6315 - val_loss: 8.9520 - val_acc: 0.1073\n",
            "Epoch 94/100\n",
            "14355/14355 [==============================] - 2s 126us/step - loss: 1.1026 - acc: 0.6391 - val_loss: 8.9753 - val_acc: 0.1112\n",
            "Epoch 95/100\n",
            "14355/14355 [==============================] - 2s 126us/step - loss: 1.1041 - acc: 0.6418 - val_loss: 9.0585 - val_acc: 0.1076\n",
            "Epoch 96/100\n",
            "14355/14355 [==============================] - 2s 125us/step - loss: 1.1021 - acc: 0.6385 - val_loss: 9.0059 - val_acc: 0.1131\n",
            "Epoch 97/100\n",
            "14355/14355 [==============================] - 2s 124us/step - loss: 1.1045 - acc: 0.6395 - val_loss: 9.0117 - val_acc: 0.1092\n",
            "Epoch 98/100\n",
            "14355/14355 [==============================] - 2s 123us/step - loss: 1.1033 - acc: 0.6377 - val_loss: 9.0896 - val_acc: 0.1095\n",
            "Epoch 99/100\n",
            "14355/14355 [==============================] - 2s 124us/step - loss: 1.0921 - acc: 0.6406 - val_loss: 9.0563 - val_acc: 0.1084\n",
            "Epoch 100/100\n",
            "14355/14355 [==============================] - 2s 125us/step - loss: 1.0906 - acc: 0.6409 - val_loss: 9.0941 - val_acc: 0.1064\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CP9U7LlOWKsx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.save(\"Language_modeling.h5\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zM9zgUMeOc-X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Model Prediction\n",
        "from keras.models import load_model\n",
        "model = load_model('Language_modeling.h5')\n",
        "Y_pred = model.predict(Xtest)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LNP450iSOoH_",
        "colab_type": "code",
        "outputId": "01f0842c-c376-42ea-b207-40434ea61b63",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 901
        }
      },
      "source": [
        "# Sample check on Test data\n",
        "print (\"Prior bigram words\", \"|Actual\", \"|Predicted\",\"\\n\")\n",
        "for i in range(50):\n",
        "  print (i,xtest_tg[i],\"|\",rev_dictnry[np.argmax(Ytest[i])], \"|\",rev_dictnry[np.argmax(Y_pred[i])])"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Prior bigram words |Actual |Predicted \n",
            "\n",
            "0 and they | re | repeated\n",
            "1 never learnt | it | left\n",
            "2 matters it | how | good\n",
            "3 as she | could | could\n",
            "4 the duchess | what | and\n",
            "5 pair of | boots | white\n",
            "6 cattle in | the | the\n",
            "7 it was | looking | the\n",
            "8 what else | have | you\n",
            "9 voice of | the | thunder\n",
            "10 to make | out | out\n",
            "11 royal children | there | and\n",
            "12 direction in | which | the\n",
            "13 in the | lap | distance\n",
            "14 offer it | said | was\n",
            "15 of onions | seven | course\n",
            "16 balanced an | eel | an\n",
            "17 tremble alice | looked | and\n",
            "18 forwards each | time | case\n",
            "19 and will | talk | have\n",
            "20 the queen | can | and\n",
            "21 thing is | to | to\n",
            "22 had nothing | yet | else\n",
            "23 tell me | please | the\n",
            "24 to get | out | in\n",
            "25 to say | word | it\n",
            "26 become of | me | it\n",
            "27 not attending | said | to\n",
            "28 about in | the | the\n",
            "29 ran the | faster | of\n",
            "30 the king | sharply | and\n",
            "31 turning purple | won | to\n",
            "32 had quite | long | to\n",
            "33 was lying | on | under\n",
            "34 up again | let | with\n",
            "35 ve had | nothing | oh\n",
            "36 on just | as | then\n",
            "37 two you | gave | re\n",
            "38 waistcoat pocket | and | the\n",
            "39 window and | on | some\n",
            "40 her way | into | they\n",
            "41 to say | she | it\n",
            "42 half to | itself | herself\n",
            "43 herself this | is | and\n",
            "44 to worry | it | grow\n",
            "45 went on | and | in\n",
            "46 march hare | alice | said\n",
            "47 that said | alice | the\n",
            "48 wanted to | send | much\n",
            "49 for the | moment | fan\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}